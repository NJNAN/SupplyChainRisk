#!/usr/bin/env python3
"""
å…¨çƒä¾›åº”é“¾é£é™©é¢„æµ‹é¡¹ç›®ä¸»å…¥å£
åŸºäºè½»é‡åŒ–RNNå’ŒGNNçš„å…¨çƒä¾›åº”é“¾é£é™©é¢„æµ‹ç³»ç»Ÿ
"""

import argparse
import sys
import os
import yaml
import logging
from datetime import datetime

def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)

    log_file = os.path.join(log_dir, f"main_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )

    return logging.getLogger(__name__)

def print_banner():
    """æ‰“å°é¡¹ç›®æ¨ªå¹…"""
    print("=" * 80)
    print("ğŸŒ å…¨çƒä¾›åº”é“¾é£é™©é¢„æµ‹ç³»ç»Ÿ")
    print("ğŸ“Š åŸºäºè½»é‡åŒ–RNNå’ŒGNNçš„é£é™©é¢„æµ‹ä¸éƒ¨ç½²ä¼˜åŒ–")
    print("ğŸ¯ è®ºæ–‡å®éªŒä»£ç  - å®Œæ•´å·¥ä½œæµç¨‹")
    print("=" * 80)

def main():
    parser = argparse.ArgumentParser(
        description="å…¨çƒä¾›åº”é“¾é£é™©é¢„æµ‹ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # è¿è¡Œå®Œæ•´æµç¨‹
  python main.py --mode all
  
  # åªè®­ç»ƒæ¨¡å‹
  python main.py --mode train
  
  # åªè¯„ä¼°æ¨¡å‹
  python main.py --mode eval
  
  # æ¨¡å‹å‹ç¼©
  python main.py --mode compress
  
  # é²æ£’æ€§æµ‹è¯•
  python main.py --mode robust
  
  # éƒ¨ç½²åŸºå‡†æµ‹è¯•
  python main.py --mode deploy
  
  # è¶…å‚æ•°ä¼˜åŒ–
  python main.py --mode optimize --model lstm --trials 50
        """
    )

    parser.add_argument(
        '--mode',
        choices=['all', 'train', 'eval', 'compress', 'robust', 'deploy', 'optimize'],
        default='all',
        help='è¿è¡Œæ¨¡å¼'
    )

    parser.add_argument(
        '--config',
        default='config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )

    parser.add_argument(
        '--model',
        choices=['rnn', 'lstm', 'gru', 'gcn', 'gat', 'graphsage', 'transformer'],
        help='æŒ‡å®šæ¨¡å‹ç±»å‹ï¼ˆç”¨äºå•æ¨¡å‹è®­ç»ƒæˆ–ä¼˜åŒ–ï¼‰'
    )

    parser.add_argument(
        '--trials',
        type=int,
        default=50,
        help='è¶…å‚æ•°ä¼˜åŒ–è¯•éªŒæ¬¡æ•°'
    )

    parser.add_argument(
        '--verbose',
        action='store_true',
        help='è¯¦ç»†è¾“å‡º'
    )

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    logger = setup_logging()

    print_banner()

    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not os.path.exists(args.config):
        logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        sys.exit(1)

    # åŠ è½½é…ç½®
    try:
        with open(args.config, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {args.config}")
    except Exception as e:
        logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)

    # åˆ›å»ºå¿…è¦ç›®å½•
    directories = ['checkpoints', 'data', 'logs', 'results']
    for dir_name in directories:
        os.makedirs(dir_name, exist_ok=True)

    logger.info(f"è¿è¡Œæ¨¡å¼: {args.mode}")

    try:
        if args.mode == 'all':
            logger.info("ğŸš€ å¼€å§‹å®Œæ•´æµç¨‹æ‰§è¡Œ...")
            run_full_pipeline(config, logger)

        elif args.mode == 'train':
            logger.info("ğŸ‹ï¸ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
            run_training(config, args.model, logger)

        elif args.mode == 'eval':
            logger.info("ğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°...")
            run_evaluation(config, logger)

        elif args.mode == 'compress':
            logger.info("ğŸ—œï¸ å¼€å§‹æ¨¡å‹å‹ç¼©...")
            run_compression(config, logger)

        elif args.mode == 'robust':
            logger.info("ğŸ›¡ï¸ å¼€å§‹é²æ£’æ€§æµ‹è¯•...")
            run_robustness_test(config, logger)

        elif args.mode == 'deploy':
            logger.info("ğŸš€ å¼€å§‹éƒ¨ç½²åŸºå‡†æµ‹è¯•...")
            run_deployment_benchmark(config, logger)

        elif args.mode == 'optimize':
            if not args.model:
                logger.error("è¶…å‚æ•°ä¼˜åŒ–æ¨¡å¼éœ€è¦æŒ‡å®šæ¨¡å‹ç±»å‹")
                sys.exit(1)
            logger.info(f"ğŸ”§ å¼€å§‹{args.model}æ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–...")
            run_hyperparameter_optimization(config, args.model, args.trials, logger)

        logger.info("âœ… æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼")
        print_results_summary()

    except KeyboardInterrupt:
        logger.info("âŒ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

def run_full_pipeline(config, logger):
    """è¿è¡Œå®Œæ•´æµç¨‹"""
    steps = [
        ("æ•°æ®å‡†å¤‡", run_data_preparation),
        ("æ¨¡å‹è®­ç»ƒ", lambda c, l: run_training(c, None, l)),
        ("æ¨¡å‹è¯„ä¼°", run_evaluation),
        ("æ¨¡å‹å‹ç¼©", run_compression),
        ("é²æ£’æ€§æµ‹è¯•", run_robustness_test),
        ("éƒ¨ç½²åŸºå‡†æµ‹è¯•", run_deployment_benchmark)
    ]

    for step_name, step_func in steps:
        logger.info(f"ğŸ“‹ æ‰§è¡Œæ­¥éª¤: {step_name}")
        step_func(config, logger)
        logger.info(f"âœ… {step_name} å®Œæˆ")

def run_data_preparation(config, logger):
    """æ•°æ®å‡†å¤‡"""
    from data_loader import load_raw_data
    from preprocessor import preprocess
    from features import extract_features

    logger.info("åŠ è½½åŸå§‹æ•°æ®...")
    raw_data = load_raw_data(config['data'])

    logger.info("æ•°æ®é¢„å¤„ç†...")
    train_df, val_df, test_df = preprocess(raw_data, config['data'])

    logger.info("ç‰¹å¾å·¥ç¨‹...")
    train_features = extract_features(train_df, config['features'])

    logger.info(f"æ•°æ®å‡†å¤‡å®Œæˆ - è®­ç»ƒé›†ç‰¹å¾ç»´åº¦: {train_features.shape}")

def run_training(config, model_type, logger):
    """è¿è¡Œè®­ç»ƒ"""
    from trainer import train

    if model_type:
        logger.info(f"è®­ç»ƒå•ä¸ªæ¨¡å‹: {model_type}")
        results = train(config, model_type)
    else:
        logger.info("è®­ç»ƒæ‰€æœ‰æ¨¡å‹...")
        results = train(config)

    logger.info("è®­ç»ƒç»“æœ:")
    for model, result in results.items():
        if 'error' not in result:
            logger.info(f"  {model}: éªŒè¯æŸå¤± = {result['final_val_loss']:.4f}")
        else:
            logger.error(f"  {model}: è®­ç»ƒå¤±è´¥ - {result['error']}")

def run_evaluation(config, logger):
    """è¿è¡Œè¯„ä¼°"""
    from evaluator import evaluate

    # æŸ¥æ‰¾è®­ç»ƒå¥½çš„æ¨¡å‹
    checkpoint_dir = config['training']['checkpoint_dir']
    model_paths = {}

    if os.path.exists(checkpoint_dir):
        for model_file in os.listdir(checkpoint_dir):
            if model_file.startswith('best_') and model_file.endswith('_model.pth'):
                model_type = model_file.replace('best_', '').replace('_model.pth', '')
                model_paths[model_type] = os.path.join(checkpoint_dir, model_file)

    if not model_paths:
        logger.warning("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè·³è¿‡è¯„ä¼°")
        return

    logger.info(f"æ‰¾åˆ° {len(model_paths)} ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°")
    results = evaluate(model_paths, config)

    if results:
        logger.info("è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° results/ ç›®å½•")

def run_compression(config, logger):
    """è¿è¡Œæ¨¡å‹å‹ç¼©"""
    from compression import compress_models

    checkpoint_dir = config['training']['checkpoint_dir']
    model_paths = {}

    if os.path.exists(checkpoint_dir):
        for model_file in os.listdir(checkpoint_dir):
            if model_file.startswith('best_') and model_file.endswith('_model.pth'):
                model_type = model_file.replace('best_', '').replace('_model.pth', '')
                model_paths[model_type] = os.path.join(checkpoint_dir, model_file)

    if not model_paths:
        logger.warning("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè·³è¿‡å‹ç¼©")
        return

    logger.info(f"å¯¹ {len(model_paths)} ä¸ªæ¨¡å‹è¿›è¡Œå‹ç¼©")
    results = compress_models(model_paths, config)

    if results:
        logger.info("æ¨¡å‹å‹ç¼©å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° results/ ç›®å½•")

def run_robustness_test(config, logger):
    """è¿è¡Œé²æ£’æ€§æµ‹è¯•"""
    from robustness import run_robustness_tests

    checkpoint_dir = config['training']['checkpoint_dir']
    model_paths = {}

    if os.path.exists(checkpoint_dir):
        for model_file in os.listdir(checkpoint_dir):
            if model_file.startswith('best_') and model_file.endswith('_model.pth'):
                model_type = model_file.replace('best_', '').replace('_model.pth', '')
                model_paths[model_type] = os.path.join(checkpoint_dir, model_file)

    if not model_paths:
        logger.warning("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè·³è¿‡é²æ£’æ€§æµ‹è¯•")
        return

    logger.info(f"å¯¹ {len(model_paths)} ä¸ªæ¨¡å‹è¿›è¡Œé²æ£’æ€§æµ‹è¯•")
    results = run_robustness_tests(model_paths, config)

    if results:
        logger.info("é²æ£’æ€§æµ‹è¯•å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° results/ ç›®å½•")

def run_deployment_benchmark(config, logger):
    """è¿è¡Œéƒ¨ç½²åŸºå‡†æµ‹è¯•"""
    from deployment import benchmark_deployment

    checkpoint_dir = config['training']['checkpoint_dir']
    model_paths = {}

    if os.path.exists(checkpoint_dir):
        for model_file in os.listdir(checkpoint_dir):
            if model_file.startswith('best_') and model_file.endswith('_model.pth'):
                model_type = model_file.replace('best_', '').replace('_model.pth', '')
                model_paths[model_type] = os.path.join(checkpoint_dir, model_file)

    if not model_paths:
        logger.warning("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè·³è¿‡éƒ¨ç½²åŸºå‡†æµ‹è¯•")
        return

    logger.info(f"å¯¹ {len(model_paths)} ä¸ªæ¨¡å‹è¿›è¡Œéƒ¨ç½²åŸºå‡†æµ‹è¯•")
    results = benchmark_deployment(model_paths, config)

    if results:
        logger.info("éƒ¨ç½²åŸºå‡†æµ‹è¯•å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° results/ ç›®å½•")

def run_hyperparameter_optimization(config, model_type, trials, logger):
    """è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–"""
    from trainer import hyperparameter_optimization

    logger.info(f"å¼€å§‹ {model_type} æ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–ï¼Œè¯•éªŒæ¬¡æ•°: {trials}")
    results = hyperparameter_optimization(config, model_type, trials)

    logger.info("è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ:")
    logger.info(f"æœ€ä½³å‚æ•°: {results['best_params']}")
    logger.info(f"æœ€ä½³å€¼: {results['best_value']}")

def print_results_summary():
    """æ‰“å°ç»“æœæ‘˜è¦"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ ç»“æœæ–‡ä»¶æ‘˜è¦")
    print("=" * 60)

    result_files = [
        ("è®­ç»ƒç»“æœ", "results/training_results.json"),
        ("è¯„ä¼°æŠ¥å‘Š", "results/evaluation_report.md"),
        ("æ¨¡å‹æ¯”è¾ƒ", "results/model_comparison.csv"),
        ("å‹ç¼©æŠ¥å‘Š", "results/compression_report.md"),
        ("é²æ£’æ€§æŠ¥å‘Š", "results/robustness_report.md"),
        ("éƒ¨ç½²æŠ¥å‘Š", "results/deployment_report.md"),
        ("æ€§èƒ½å›¾è¡¨", "results/*.png")
    ]

    for name, path in result_files:
        if "*" in path:
            # é€šé…ç¬¦æ–‡ä»¶ï¼Œæ£€æŸ¥ç›®å½•
            import glob
            files = glob.glob(path)
            if files:
                print(f"âœ… {name}: {len(files)} ä¸ªæ–‡ä»¶")
            else:
                print(f"âŒ {name}: æœªç”Ÿæˆ")
        else:
            if os.path.exists(path):
                print(f"âœ… {name}: {path}")
            else:
                print(f"âŒ {name}: æœªç”Ÿæˆ")

    print("=" * 60)
    print("ğŸ‰ å®éªŒå®Œæˆï¼è¯·æŸ¥çœ‹ results/ ç›®å½•è·å–è¯¦ç»†ç»“æœ")
    print("=" * 60)

if __name__ == "__main__":
    main()
